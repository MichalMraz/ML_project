{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michalmraz/Desktop/ML_project/ml_project_venv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "# from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # slower but deterministic\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_causal_mask(seq_len):\n",
    "    # Create a matrix with ones in the lower triangle, zeros above\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask  # shape (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        self.scale = d_k ** 0.5\n",
    "\n",
    "    def forward(self, Q, K, V, causal_mask=False, padding_mask=None):\n",
    "        # Q, K, V: (batch_size, num_heads, seq_len, d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  # (B, H, L, L)\n",
    "\n",
    "        if causal_mask:\n",
    "            causal_mask = generate_causal_mask(scores.size(-1)).expand(scores.size(0), scores.size(1), -1, -1).to(scores.device)\n",
    "            scores = scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "        if padding_mask is not None:\n",
    "            padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "            scores = scores.masked_fill(padding_mask == 0, float('-inf'))  # Mask out pad positions\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # (B, H, L, L)\n",
    "        output = torch.matmul(attention_weights, V)    # (B, H, L, d_k)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_embedding, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_embedding % num_heads == 0\n",
    "        self.d_k = d_embedding // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        #in reality these can map to a lower dimensional space to make things faster``\n",
    "        self.W_q = nn.Linear(d_embedding, d_embedding)\n",
    "        self.W_k = nn.Linear(d_embedding, d_embedding)\n",
    "        self.W_v = nn.Linear(d_embedding, d_embedding)\n",
    "        self.W_o = nn.Linear(d_embedding, d_embedding)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(self, x, causal_mask=False, padding_mask=None):\n",
    "        x_input = x\n",
    "        x = self.norm(x)\n",
    "\n",
    "        B, L, d_embedding = x.size()  # Batch, Sequence Length, Embedding Dim\n",
    "        H = self.num_heads\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.W_q(x).view(B, L, H, self.d_k).transpose(1, 2)  # (B, H, L, d_k)\n",
    "        K = self.W_k(x).view(B, L, H, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, L, H, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply attention\n",
    "        context = self.attention(Q, K, V, causal_mask, padding_mask)  # (B, H, L, d_k)\n",
    "\n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L, d_embedding)  # (B, L, d_embedding)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_o(context)  # (B, L, d_embedding)\n",
    "\n",
    "        # Add (& pre-Norm)\n",
    "        #my preference is to do pre-norm for better stabiliy, even though the original paper used post-norm\n",
    "        output = x_input + output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_embedding, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_embedding % num_heads == 0\n",
    "        self.d_k = d_embedding // num_heads\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_embedding, d_embedding)\n",
    "        self.W_k = nn.Linear(d_embedding, d_embedding)\n",
    "        self.W_v = nn.Linear(d_embedding, d_embedding)\n",
    "        self.W_o = nn.Linear(d_embedding, d_embedding)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "\n",
    "    def forward(self, x_decoder, x_encoder, causal_mask=False, padding_mask=None):\n",
    "        assert x_decoder.size() == x_encoder.size() #x and x_encoder must have the same size\n",
    "        x_input = x_decoder\n",
    "        x_decoder = self.norm(x_decoder)\n",
    "        \n",
    "        B, L, d_embedding = x_decoder.size()  # Batch, Sequence Length, Embedding Dim\n",
    "        H = self.num_heads\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.W_q(x_decoder).view(B, L, H, self.d_k).transpose(1, 2)  # (B, H, L, d_k)\n",
    "        K = self.W_k(x_encoder).view(B, L, H, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x_encoder).view(B, L, H, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply attention\n",
    "        context = self.attention(Q, K, V, causal_mask, padding_mask)  # (B, H, L, d_k)\n",
    "\n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L, d_embedding)  # (B, L, d_embedding)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_o(context)  # (B, L, d_embedding)\n",
    "\n",
    "        output = output + x_input\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_embedding, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_embedding, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_embedding)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(d_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        x = self.norm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = x_input + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_embedding, num_heads, d_ff, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadAttention(d_embedding, num_heads),\n",
    "            FeedForwardNetwork(d_embedding, d_ff)\n",
    "        ] * num_layers)\n",
    "    \n",
    "    def forward(self, x, causal_mask=False, padding_mask=None):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i % 2 == 0:\n",
    "                x = layer(x, causal_mask=causal_mask, padding_mask=padding_mask)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoder, d_embedding, num_heads, d_ff, num_layers, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.layers = nn.ModuleList([\n",
    "            MultiHeadAttention(d_embedding, num_heads),\n",
    "            MultiHeadCrossAttention(d_embedding, num_heads),\n",
    "            FeedForwardNetwork(d_embedding, d_ff)\n",
    "        ] * num_layers)\n",
    "        self.linear = nn.Linear(d_embedding, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x_decoder, x_encoder, causal_mask=False, encoder_padding_mask=None, decoder_padding_mask=None, inference=False):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i % 3 == 0:\n",
    "                x_decoder = layer(x_decoder, causal_mask=causal_mask, padding_mask=decoder_padding_mask)\n",
    "            elif i % 3 == 1:\n",
    "                x_decoder = layer(x_decoder, x_encoder, causal_mask=False, padding_mask=encoder_padding_mask)\n",
    "            else:\n",
    "                x_decoder = layer(x_decoder)\n",
    "        x_decoder = self.linear(x_decoder)\n",
    "        if inference:\n",
    "            x_decoder = self.softmax(x_decoder)\n",
    "        return x_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_embedding, max_len): \n",
    "        #max_len is the maximum length of the input sequence\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Parameter(torch.randn(vocab_size, d_embedding))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_embedding)\n",
    "        powers = torch.repeat_interleave(torch.arange(0, 1, 2/d_embedding), repeats=2).expand(max_len, -1)\n",
    "        divisors = torch.pow(10000, powers)\n",
    "        positions = torch.arange(0, max_len).view(max_len, -1).expand(-1, d_embedding)\n",
    "        args = positions / divisors\n",
    "        pe[:, 0::2] = torch.sin(args[:, 0::2])\n",
    "        pe[:, 1::2] = torch.cos(args[:, 1::2])\n",
    "\n",
    "        # Register as buffer so it's not a parameter but moves with `.to(device)`\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, seq_len, d_embedding)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # Add positional encoding: broadcast over batch dimension\n",
    "        x = self.embedding[x] + self.pe[:seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(padded_input, pad_token_id=0):\n",
    "    return (padded_input != pad_token_id).int()  # shape: (batch, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_embedding, num_heads, d_ff, num_layers, max_len):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding(vocab_size, d_embedding, max_len)\n",
    "        self.encoder = Encoder(d_embedding, num_heads, d_ff, num_layers)\n",
    "        self.decoder = Decoder(self.encoder, d_embedding, num_heads, d_ff, num_layers, vocab_size)\n",
    "\n",
    "    def forward(self, x_encoder, x_decoder, causal_mask=False):\n",
    "        encoder_padding_mask = create_padding_mask(x_encoder)\n",
    "        decoder_padding_mask = create_padding_mask(x_decoder)\n",
    "\n",
    "        x_encoder = self.embedding(x_encoder)\n",
    "        x_decoder = self.embedding(x_decoder)\n",
    "        x_encoder = self.encoder(x_encoder, causal_mask=False, padding_mask=encoder_padding_mask)\n",
    "        output = self.decoder(x_decoder, x_encoder, causal_mask=causal_mask, encoder_padding_mask=encoder_padding_mask, decoder_padding_mask=decoder_padding_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset('opus_books', 'en-sk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the name of the model whose tokenizer we are using. We will need it later.\n",
    "PRE_TRAINED_MODEL_NAME = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "\n",
    "# Download the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_have_presaved_data = False\n",
    "#only set to true if I don't have 'tokenized_dataset_tensor.pt' in the current folder. Takes 15 min in total to download and tokenize the dataset\n",
    "\n",
    "if dont_have_presaved_data:\n",
    "    url = \"http://www.statmt.org/europarl/v7/sk-en.tgz\"\n",
    "    filename = \"sk-en.tgz\"\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        r = requests.get(url)\n",
    "        with open(filename, \"wb\") as w:\n",
    "            w.write(r.content)\n",
    "        print(\"File downloaded\")\n",
    "    else:\n",
    "        print(\"File already exists\")\n",
    "    # Open a gzipped tar file in read mode\n",
    "    with tarfile.open(\"sk-en.tgz\", \"r:gz\") as tar:\n",
    "        # Get a TarFile object for the archive\n",
    "        # Extract all the files and directories to the current folder\n",
    "        tar.extractall()\n",
    "    # Define a function for text cleaning\n",
    "    def clean_text(text):\n",
    "\n",
    "        # Convert text to lowercase\n",
    "        text = str(text).lower().strip()\n",
    "\n",
    "        # Remove the \\n at the end of each line in the file\n",
    "        text = text.rstrip('\\n')\n",
    "\n",
    "        # Remove HTML tags and non-alphanumeric characters\n",
    "        text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "        text = re.sub(r\"[^a-zA-ZÀ-ž0-9\\s.,;!?':()\\[\\]{}-]\", \" \", text)  # Keep selected punctuation marks, symbols and apostrophes\n",
    "\n",
    "        # Remove excessive whitespace (more than one space)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        text = text.encode(\"utf-8\", errors=\"ignore\").decode(\"utf-8\")  # Corrected encoding\n",
    "\n",
    "        return text\n",
    "    \n",
    "    # Read from the two text files, and create a pandas Series object (like a python list[]), and clean the text by\n",
    "    # applying our clean_text function to every element.\n",
    "\n",
    "    with open('europarl-v7.sk-en.en', 'r') as en_file, open('europarl-v7.sk-en.sk', 'r') as sk_file:\n",
    "        en = pd.Series(en_file.readlines(), name='en').apply(lambda text: clean_text(text))\n",
    "        sk = pd.Series(sk_file.readlines(), name='sk').apply(lambda text: clean_text(text))\n",
    "    translation_df = pd.concat([en, sk], axis=1)\n",
    "\n",
    "    # this takes over 10 min so I save the result and load it\n",
    "    tokenized = [{\n",
    "        'en': tokenizer.encode(translation_df.loc[i,'en'], return_tensors='pt', padding='max_length', max_length=max_len, truncation=True, add_special_tokens=True),\n",
    "        'sk': tokenizer.encode(translation_df.loc[i,'sk'], return_tensors='pt', padding='max_length', max_length=max_len, truncation=True, add_special_tokens=True)\n",
    "    } for i in range(translation_df.shape[0])]\n",
    "\n",
    "\n",
    "    # First, separate English and Slovak tensors\n",
    "    en_tensors = [item['en'] for item in tokenized]  \n",
    "    sk_tensors = [item['sk'] for item in tokenized]  \n",
    "\n",
    "    # Stack them into 3D tensors (num_sentences, max_len)\n",
    "    en_tensor = torch.stack(en_tensors)\n",
    "    sk_tensor = torch.stack(sk_tensors)\n",
    "    tokenized_dataset_tensor = torch.stack([en_tensor, sk_tensor])\n",
    "    torch.save(tokenized_dataset_tensor, 'tokenized_dataset_tensor.pt')\n",
    "else:\n",
    "    tokenized_dataset_tensor = torch.load('tokenized_dataset_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "vocab_size = tokenizer.vocab_size #119547\n",
    "max_len = 128\n",
    "d_embedding = 256\n",
    "d_ff = 1024\n",
    "num_heads = 8\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input extras\n",
    "seq_len = 5\n",
    "batch_size = 2\n",
    "\n",
    "x_test = torch.randn(batch_size, seq_len, d_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_embedding=d_embedding,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers,\n",
    "    max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_len in dataset is 566 but I'm happy to go with 512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TranslationDataset(Dataset): #this is for the tokenized dataset (old)\n",
    "#     def __init__(self, tokenized):\n",
    "#         self.tokenized = tokenized\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.tokenized)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return (self.tokenized[idx]['en'], self.tokenized[idx]['sk'])\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# # tokenizer.pad_token = tokenizer.eos_token\n",
    "# # dataset = TranslationDataset(examples, tokenizer)\n",
    "# dataset = TranslationDataset(tokenized)\n",
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset_tensor):\n",
    "        self.tokenized_dataset_tensor = tokenized_dataset_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tokenized_dataset_tensor.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.tokenized_dataset_tensor[0][idx], self.tokenized_dataset_tensor[1][idx])\n",
    "        # 0 for English, 1 for Slovak\n",
    "\n",
    "dataset = TranslationDataset(tokenized_dataset_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cel = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-9, lr=1e-4) #betas and eps from the original paper\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-4) #betas and eps from the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_lambda(step_num): #lr decay from original paper. We use fewer warmups\n",
    "    step_num = step_num + 1 #to avoid division by zero warning at step 0\n",
    "    warmup_steps = 10\n",
    "    return min(1/np.sqrt(step_num), step_num / math.pow(warmup_steps, 1.5)) / np.sqrt(d_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer = transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "# we don't use dropout and label smoothing and warmup lr yet. Implement when we have a larger dataset and want to run a larger model with long training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/40045 [00:40<452:25:45, 40.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 12.684313774108887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/40045 [01:18<433:44:13, 38.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 12.442429542541504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/40045 [02:01<454:11:45, 40.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 11.994535446166992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/40045 [02:39<592:35:46, 53.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t4/jszkcmcn7lvgmswvxnpyh1x00000gn/T/ipykernel_715/702187804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_project/ml_project_venv/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_project/ml_project_venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for batch_data, batch_labels in tqdm(dataloader):\n",
    "    count += 1\n",
    "    # print(batch_data.shape, batch_labels.shape)\n",
    "    batch_data = batch_data.squeeze(1).to(device)  # Remove the extra dimension\n",
    "    batch_labels = batch_labels.squeeze(1).to(device)  # Remove the extra dimension\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(batch_data, batch_data, causal_mask=True)\n",
    "    loss = cel(output.permute(0, 2, 1), batch_labels)\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "    print(f\"Batch Loss: {loss.item()}\")\n",
    "    if count % 5000 == 0:\n",
    "        torch.save(transformer.state_dict(), f\"transformer_batch_{count}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"transformer_weights_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = {}\n",
    "batches = [5000*i for i in range(1, 5)]\n",
    "for batch in batches:\n",
    "\n",
    "    transformers[batch] = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_embedding=d_embedding,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers,\n",
    "    max_len=max_len)\n",
    "    transformers[batch].load_state_dict(torch.load(f\"transformer_batch_{batch}.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40045 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000 Loss: 14.123443603515625\n",
      "Batch 10000 Loss: 14.123443603515625\n",
      "Batch 15000 Loss: 14.123443603515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/40045 [01:40<1117:10:30, 100.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20000 Loss: 14.123443603515625\n",
      "Batch 5000 Loss: 14.147550582885742\n",
      "Batch 10000 Loss: 14.147550582885742\n",
      "Batch 15000 Loss: 14.147550582885742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/40045 [03:17<1096:11:58, 98.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20000 Loss: 14.147550582885742\n",
      "Batch 5000 Loss: 14.37140941619873\n",
      "Batch 10000 Loss: 14.37140941619873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/40045 [04:24<1473:23:18, 132.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t4/jszkcmcn7lvgmswvxnpyh1x00000gn/T/ipykernel_1444/901874645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch {batch} Loss: {loss.item()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_project/ml_project_venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_project/ml_project_venv/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/ML_project/ml_project_venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch_data, batch_labels in tqdm(dataloader):\n",
    "    for batch in batches:\n",
    "    # transformers[batch].to(device)\n",
    "        transformers[batch].eval()\n",
    "        batch_data = batch_data.squeeze(1).to(device)  # Remove the extra dimension\n",
    "        batch_labels = batch_labels.squeeze(1).to(device)  # Remove the extra dimension\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = transformers[batch](batch_data, batch_data, causal_mask=True)\n",
    "        loss = cel(output.permute(0, 2, 1), batch_labels)\n",
    "        print(f\"Batch {batch} Loss: {loss.item()}\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.load_state_dict(torch.load(\"transformer_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transformer.load_state_dict(torch.load(\"transformer_batch_0_test.pth\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the inference is still mega crap. Figure out how to look into it at least a bit better, to see if indeed at least some progress is being made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should use causal mask in decoder self attention at inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, tokenizer, device, max_len=max_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize input sentence and convert to tensor\n",
    "        # input_ids = tokenizer.encode(sentence, return_tensors=\"pt\").to(device)\n",
    "        input_ids = tokenizer(sentence, return_tensors='pt', padding='max_length', max_length=max_len, truncation=True)['input_ids']\n",
    "\n",
    "        # Create attention mask (1 for non-pad tokens)\n",
    "        encoder_padding_mask = create_padding_mask(input_ids, pad_token_id=0)\n",
    "\n",
    "        # Pass through encoder\n",
    "        input_ids = model.embedding(input_ids)\n",
    "        encoder_outputs = model.encoder(input_ids, padding_mask=encoder_padding_mask)\n",
    "\n",
    "        # Initialize decoder input with BOS token\n",
    "        decoder_input = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "\n",
    "        translated_tokens = []\n",
    "\n",
    "        for i in range(max_len):\n",
    "            # Create causal mask and decoder attention mask if needed\n",
    "            decoder_padding_mask = create_padding_mask(decoder_input)\n",
    "            decoder_input_embedded = model.embedding(decoder_input)\n",
    "            filler = torch.zeros(encoder_outputs.size(0), encoder_outputs.size(1) - decoder_input_embedded.size(1), encoder_outputs.size(2))\n",
    "            decoder_input_embedded = torch.cat([decoder_input_embedded, filler], dim=1)\n",
    "            filler = torch.zeros(decoder_padding_mask.size(0), encoder_padding_mask.size(1) - decoder_padding_mask.size(1))\n",
    "            decoder_padding_mask = torch.cat([decoder_padding_mask, filler], dim=1)\n",
    "            # decoder_input_embedded = decoder_input_embedded.expand(encoder_outputs.size())\n",
    "\n",
    "            # Forward pass through model\n",
    "            outputs = model.decoder(\n",
    "                decoder_input_embedded,\n",
    "                encoder_outputs,\n",
    "                causal_mask=True,\n",
    "                decoder_padding_mask=decoder_padding_mask,\n",
    "                encoder_padding_mask=encoder_padding_mask\n",
    "            )\n",
    "\n",
    "            # Output logits for the last time step\n",
    "            # logits = model.output_layer(outputs)  # shape: (1, seq_len, vocab_size)\n",
    "            logits = outputs\n",
    "            next_token_logits = logits[:, i, :]  # (1, vocab_size)\n",
    "\n",
    "            # Pick token with highest probability\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).item()\n",
    "\n",
    "            # Append predicted token\n",
    "            translated_tokens.append(next_token_id)\n",
    "\n",
    "            # Break if EOS\n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # Update decoder input\n",
    "            decoder_input = torch.cat(\n",
    "                [decoder_input, torch.tensor([[next_token_id]], device=device)], dim=1\n",
    "            )\n",
    "\n",
    "        # Decode tokens into sentence\n",
    "        translated_sentence = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
    "        return translated_sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for my local work \n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t4/jszkcmcn7lvgmswvxnpyh1x00000gn/T/ipykernel_62665/1973894423.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I am cool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Translation:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/t4/jszkcmcn7lvgmswvxnpyh1x00000gn/T/ipykernel_62665/1469369681.py\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(sentence, model, tokenizer, device, max_len)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Initialize decoder input with BOS token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_token_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtranslated_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "translated = translate_sentence(\"I am cool\", transformer, tokenizer, device=\"cpu\")\n",
    "print(\"Translation:\", translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project idea \\\n",
    "it can predict sequence of numbers, in words. \\\n",
    "two four six eight ten - twelve \\\n",
    "three six nine twelve - fifteen \\\n",
    "could be arithmetic and geometric. I will generate them, code up the number to string mapper, pass it mapped to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
